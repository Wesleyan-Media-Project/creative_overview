# creative_overview

An overview of all repos belonging to the CREATIVE project

<br>

## Summary of Working Repositories


---

## [Issue classifier](https://github.com/Wesleyan-Media-Project/issue_classifier)

Summary: "Issue classifier, trained on 2018 and 2020 ads - both TV and Facebook, designed to be applied to uncoded 2022 ads. Based on WMP issue coding - not Kantar."

---
## [face_url_scraper_2022](https://github.com/Wesleyan-Media-Project/face_url_scraper_2022)
`face_url_scraper_2022` is a Python-based web scraper that extracts images of political figures from URLs and saves the results in a CSV file.

### Dependencies
Some scripts run in a Python environment using Jupyter Notebook and require the following packages:
- pandas
- numpy
- bs4
- urllib
- re
- fuzzywuzzy

In addition, some parts of the code require R to be installed, along with the following R packages:
- tidyverse
- rvest
- httr
- xml2

---

## [datasets](https://github.com/Wesleyan-Media-Project/datasets)

`datasets` serves as a storage location for datasets used in other repositories.

### Some other repositories that use this repository
- [forum_digital_2022](https://github.com/Wesleyan-Media-Project/forum_digital_2022)
- [ad_goal_classifier](https://github.com/Wesleyan-Media-Project/ad_goal_classifier)
- [ad_tone](https://github.com/Wesleyan-Media-Project/ad_tone)
- [forum_digital_2022](https://github.com/Wesleyan-Media-Project/forum_digital_2022)
- [ad_goal_classifier](https://github.com/Wesleyan-Media-Project/ad_goal_classifier)
- [ABSA](https://github.com/Wesleyan-Media-Project/ABSA)
- [entity_linking_2022](https://github.com/Wesleyan-Media-Project/entity_linking_2022)
- [race_of_focus](https://github.com/Wesleyan-Media-Project/race_of_focus)
- [issue_classifier](https://github.com/Wesleyan-Media-Project/issue_classifier)
- [party_classifier](https://github.com/Wesleyan-Media-Project/party_classifier)
### Dependencies
 Some parts of the code in `datasets` require R to be installed, along with the following R libraries:
 - data.table
 - dplyr
 - tidyr 
 - stringr

 ---
## [entity_linking_2022](https://github.com/Wesleyan-Media-Project/entity_linking_2022)
 This repository is an entity linker for 2022 election data. This entity linker was trained on data that contains descriptions of people and their names, along with their aliases. Data are sourced from the 2022 WMP persons file, and are restricted to general election candidates and other non-candidate persons of interest (sitting senators, cabinet members, international leaders, etc.)

### Dependencies
Some scripts run in a Python environment require the following packages:
- csv
- pathlib
- os
- random
- json
- pandas
- spacy (***requires version 3.2***)
- en_core_web_lg from spacy (***requires manual instalation through `python -m spacy download en_core_web_lg`***)
- KnowledgeBase from spacy.kb
- minibatch and compounding from spacy.util
- tqdm
- numpy
- Example from spacy.training
- load_kb from spacy.ml.models

In addition, some parts of the code require R to be installed, along with the following R libraries: 
- dplyr
- data.table
- haven
- stringr
- stringi
- quanteda
- tidyr

---
## [ABSA](https://github.com/Wesleyan-Media-Project/ABSA)

This repository contains code for the Aspect-Based Sentiment Analysis (ABSA) project. The repository contains data, model and script for ABSA training. The classification report generated by script "02_train_rf_118m.py" shows that the ABSA model has an overall accuracy of 0.81, with a macro average f1-score of 0.68.

### Dependencies
Some scripts run in a Python environment require the following packages:
- pandas
- sklearn.feature_extraction.text
- sklearn.ensemble (requires `RandomForestClassifier` and `Pipeline` )
- sklearn
- joblib

In addition, some parts of the code require R to be installed, along with the following R libraries: 
- data.table
- tidyr
- stringi
- stringr
- dplyr

---
## [ad_tone](https://github.com/Wesleyan-Media-Project/ad_tone)

This repository contains models to predict the tone of political ads. The main model "ad_tone_constructed" is based on this [flowchart](https://docs.google.com/presentation/d/11E9kX1oVYfMooTdD1GAJfwJtdPIQpYB3lJ7i5e83ZEw/edit#slide=id.g1062def0ba3_0_0).
### Dependencies
 To run the script that constructs this variable, the following packages are required:
 - [ABSA](https://github.com/Wesleyan-Media-Project/ABSA), 
 - [race of focus](https://github.com/Wesleyan-Media-Project/race_of_focus),
- candidates dataset
- `mention-based ad tone` 

In addition, some parts of the code require R to be installed, along with the following R libraries: 
- data.table
- tidyverse
- dplyr
- purrr
- stringr

---
## [ad_goal_classifier](https://github.com/Wesleyan-Media-Project/ad_goal_classifier)
The purpose of this repository is to classify the goals of advertisements across different data sources, including Facebook, TV, and Google. It involves a series of scripts that clean and prepare the data, train a machine learning model, and apply the trained model to different data sets for inference. 

For more details on the project pipeline, refer to the [README](https://github.com/Wesleyan-Media-Project/ad_goal_classifier/blob/main/README.md) in the repository.

### Dependencies
Some scripts run in a Python environment require the following packages:
- sklearn
- pandas
- numpy
- joblib

You can install these libraries by running the following command:

`pip install scikit-learn pandas numpy joblib`

In addition, some parts of the code require R to be installed, along with the following R libraries:
- data.table
- stringr
- stringi
- dplyr
- tidyr

---
## [attack_like](https://github.com/Wesleyan-Media-Project/attack_like)
This repository contains code for an attack-like negativity measure applied to the 1.18 million Facebook dataset "fb_2020_attacklike.csv". The classifier outputs class labels of either "Support" or "Attack" as well as class probabilities, which can be used to create a "Contrast" label. The classifier is trained on 7,949 political advertisements from a 2018 Facebook data set and uses a pre-trained DistilBERT base model to fine-tune the model for the classification of ad tone. 

For more details on the classifier, refer to the [README](https://github.com/Wesleyan-Media-Project/attack_like#readme) in the repository.


The code "1_embed_apsr.py" is about sentiment analysis of text data using the BERT (Bidirectional Encoder Representations from Transformers) model. The goal of this code is to build a model to predict the sentiment (either Attack, Promote or Contrast) of the text based on its content. The following packages are required to run this code:

- numpy
- pandas
- pickle
- scikit-learn
- torch
- transformers

"2_embed_fb2020.py" preprocesses a Facebook ad dataset and extracts sentiment embeddings from the ad texts using the Pretrained DistilBERT Model by huggingface (Transformers library).
This code requires the following Python packages:

- numpy
- pandas
- torch
- transformers
- warnings
- time

---
## [fb_2020](https://github.com/Wesleyan-Media-Project/fb_2020)
This repository contains codes that load, merge and process data from multiple sources to create a single comprehensive dataset.The data sets are in the form of CSV (Comma-Separated Value) files, and the code uses the Pandas library to load and manipulate them. More information about the datasets, variables and output files can be found in the [README](https://github.com/Wesleyan-Media-Project/fb_2020#readme)

### Dependencies
Some scripts run in a Python environment using Jupyter Notebook and require the following packages:
- pandas
- numpy
- ast
- re
